# LLMs From Scratch
This repository is a structured deep-dive into implementing a Large Language Model from scratch â€” inspired by Stanfordâ€™s [CS336: Large Language Models](https://stanford-cs336.github.io/) and Karpathyâ€™s [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0)

It combines theory + code, building all components from low-level tokenizers to Transformer architectures, training loops, and sampling strategies.
Some of the blogs related to this repo are on my [website](https://timashov.ai).

## ðŸ“š Table Of Contents
- [1. Core](#core)
  - [Tokenizers](#tokenizers)
  - [Optimizers](#optimizers)
  - [Layers](#layers)
- [2. Efficiency](#efficiency)
- [3. Scaling](#scaling)
- [4. Data Pipelines](#data)
- [5. Finetuning](#finetuning)
- [6. Alignment](#alignment)
- [7. Inference](#inference)

## ðŸ“˜ Resources Used

This repo builds upon:
- ðŸ“˜ [CS224N â€“ NLP with Deep Learning (Stanford)](https://web.stanford.edu/class/cs224n/)
- ðŸ“˜ [CS336 â€“ Large Language Models (Stanford)](https://stanford-cs336.github.io/)
- ðŸ“º [Karpathy â€“ Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
