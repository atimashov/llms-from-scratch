# Natural Language Processing
In this repo, I implement foundational and cutting-edge papers in NLP, from classic word embeddings to large multimodal models.

## ðŸ“š Table Of Contents
- [1. NLP Refresher: Pre-Transformer Ideas](#nlp-refresher)
  - [Word Embeddings](./p1_pre_transformer/word_embeddings/README.md)
    <br>Foundational techniques for learning word embeddings: **Word2Vec** and **GloVe**.
  - [Optimizers](#optimizers)
  - [Tokenizers](#tokenizers)
  - [Recurrent Neural Networks](#rnn)
  - [Sequence to Secuence](#seq2seq) 
- [2. Transformers](#transformers)
- [3. LLMs](#llms)
- [4. VLMs](#vlms)

## ðŸ“˜ Resources Used

This repo builds upon:
- ðŸ“˜ [CS224N â€“ NLP with Deep Learning (Stanford)](https://web.stanford.edu/class/cs224n/)
- ðŸ“˜ [CS336 â€“ Large Language Models (Stanford)](https://stanford-cs336.github.io/)
- ðŸ“º [Karpathy â€“ Neural Networks: Zero to Hero](https://www.youtube.com/@karpathy)
