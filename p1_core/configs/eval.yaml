seed: 123
device: 0
batch_size: 64
model:
  vocab_size: 32_000
  context_length: 256
  d_model: 512 # 768 (dimensions used in many small Transformer papers)
  d_ff: 1344 # ~8/3 d_model; 64x, which is good for GPU performance.
  activation: "SiLU"
  is_gate: True
  rope_theta: 10_000
  num_layers: 4
  num_heads: 16 # 17M non-embedding parameters
  norms: # can be either 'RMSNorm', 'LayerNorm', None
    before: 'RMSNorm'
    after: 'RMSNorm'
    residual: Null
    final: 'RMSNorm'
  dtype: amp
  load_prefix: "~/ai_projects/llms-from-scratch/p1-core/weights/OpenWebText"
  load_name: "exp_bs_64/cosine/steps_20000/warmup_1000/Lion/lrmax0.0003_lrmin2.9999999999999997e-05_wdecay0.005/dmodel_512_dff_1344_numlayers_4_numheads_16_cntx_256/20250820_112759/ckpt_best.pt"
data:
  num_samples: -1
  path: '~/ai_projects/data/TinyStoriesV2-GPT4/tokenized/valid.npy'