seed: 123
dataset_path:
  prefix: '~/ai_projects/data/TinyStoriesV2-GPT4'
  tokenized: 'tokenized'
device: 0 # currently no distributed training
model: # 17M non-embedding parameters
  vocab_size: 10_000
  context_length: 256 # 256 # 512
  d_model:  512 # 512   # ( 768 is a dimension used in many small Transformer papers)
  d_ff:  1344 # 1344 # 2016 # 3072 # 6144   ~8/3 d_model; 64x, which is good for GPU performance.
  activation: "SiLU"
  is_gate: true
  rope_theta: 10_000
  num_layers: 4
  attention:
    type: 'gqa'
    num_heads: 16
    num_heads_kv: 4
    d_latent: 128 # as usual 4 * d_h
  dtype: 'amp'
  mixed_precision_dtype: 'bfloat16'
  inits:
    type_ff: 'xavier'
    std_emb: 1.0
    clip_w: 3.0
  norms: # can be either 'RMSNorm', 'LayerNorm', null
    before: Null
    after: 'RMSNorm'
    residual: Null
    final: 'RMSNorm'
  weights_tying: False
optimizer:
  name: 'AdamW' # 'Lion' # 
  lr: 2e-3 # 7e-5 #  
  beta1: 0.9 # 0.92 # 
  beta2: 0.98 # 0.92 # 
  beta3: 0.99
  epsilon: 1e-8
  weight_decay: 0.1
  nesterov: false
  is_trust_ratio: false
  clip_gradient:
    max_norm: 1.0
scheduler:
  name: 'cosine'
  lr_min: 2e-4 # 7e-6
  warmup_iters: 100 # 0.03
  flat_iters: 0
  cosine_cycle_iters: 1.0
loss:
  name: 'cross_entropy'
  z_alpha: 1e-4
train:
  total_tokens_processed: 81_920_000 # 327_680_000 # 655_360_000 #  1_638_400_000 # 1_228_800_000 # 409_600_000 # 819_200_000 #   983_040_000  batch_size x step_count x context_length
  total_flops: Null # 4e17 # 2e16 # null # 4 * 10**16
  batch_size: 64
  optim_step_batch_size: 64 # 2560 # 1280
  loader_mode: 'in_memory_ids' # 'sample' # ,
  compile: true
  # ckpt_load_from: 'checkpoints/OpenWebText/wandb/20250921_200000_sunday/ckpt_best.pt'
  resume_training: false  # true: resume optimizer etc., false: warm-start weights only
validate:
  monitor:
    frequency: 0.1
  checkpoint:
    min_interval_steps: 1000    # At least this many steps between checkpoints
    num_samples: 1280
    near_end:
      threshold: 5000           # Switch to more frequent checking near end
      min_steps: 50
      max_steps: 500
  report:
    steps_ratio: {0.5} # {0.25, 0.5, 0.75}
    num_samples: 32_000
serialize:
  path: "checkpoints"
  postfix: ""
debug: false
logger: 'wandb'