seed: 123
dataset_path:
  prefix: '~/ai_projects/data/OpenWebText'
  # raw_data: 'raw_data'
  tokenized: 'tokenized'
device: 0 # currently no distributed training
model:
  vocab_size: 32_000
  context_length: 256 # 256 # 512
  d_model:  512 # 1024 # 768 #    (dimensions used in many small Transformer papers)
  d_ff:  1344 # 2688 # 2016 # 3072 # 6144   ~8/3 d_model; 64x, which is good for GPU performance.
  activation: "SiLU"
  is_gate: True
  rope_theta: 10_000
  num_layers: 4 # 4
  num_heads: 16 # 16 # 17M non-embedding parameters
  dtype: 'amp'
  norms: # can be either 'RMSNorm', 'LayerNorm', None
    before: 'RMSNorm'
    after: 'RMSNorm'
    residual: Null
    final: 'RMSNorm'
  weights_tying: False
  # load_prefix: "~/ai_projects/Natural_Language_Processing/p2_transformers/weights/TinyStoriesV2-GPT4/RTX5090"
  # load_name: "exp_bs_64_step_bs_64/loss_init/cosine/steps_40000/warmup_2000/Lion/lrmax5e-4_lrmin5e-5_wdecay5e-4/dmodel_512_dff_1344_numlayers_4_numheads_16_cntx_256/20250830_203010/ckpt_best.pt"
  # load_name: "exp_bs_64_step_bs_64/loss_init/cosine/steps_40000/warmup_2000/Lion_tr/lrmax3e-3_lrmin3e-4_wdecay5e-4/dmodel_512_dff_1344_numlayers_4_numheads_16_cntx_256/20250830_203128/ckpt_best.pt"
optimizer:
  name:  'Lion' # 'AdamW' # 'Lion' #
  lr: 1e-4
  beta: 0.9
  beta1: 0.9 # 0.98 # 
  beta2: 0.999 # 0.92 # 
  beta3: 0.99
  epsilon: 1e-8
  weight_decay: 5e-5
  nesterov: False
  is_trust_ratio: False
  scheduler:
    name: cosine
    lr_min: 7e-6
    warmup_iters: 1200 # 0.03
    cosine_cycle_iters: 1.0
  clip_gradient:
    max_norm: 1.0
train:
  total_tokens_processed: 327_680_000 # 1_638_400_000 # 1_228_800_000 # 409_600_000 # 819_200_000 #   655_360_000 #  983_040_000  batch_size x step_count x context_length
  batch_size: 64
  optim_step_batch_size: 128 # 2560 # 1280
  loader_mode: 'in_memory_ids' # 'sample' # ,
  z_alpha: 1e-4
  compile: True
validate:
  frequency: 0.1 # assumed that we don't have gradient accumulation
  eval_steps: {0.5} # {0.25, 0.5, 0.75}
  num_samples: 32_000 # '-1' means the full validation set
serialize: # and heavy_eval
  path: "weights"
  steps: 1000
  min_steps: 50
  near_end_threshold: 5000
  num_samples: 1280
  frequency: 0.1 # assumed that we don't have gradient accumulation
  postfix: ""
debug: False
