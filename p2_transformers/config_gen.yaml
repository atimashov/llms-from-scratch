seed: 123
device: "cpu" #0 # currently no distributed training
model:
  vocab_size: 10_000
  context_length: 256
  d_model: 512 # 768 (dimensions used in many small Transformer papers)
  d_ff: 1344 # ~8/3 d_model; 64x, which is good for GPU performance.
  rope_theta: 10_000
  num_layers: 4
  num_heads: 16 # 17M non-embedding parameters
  dtype: float32
  load_prefix: "~/ai_projects/Natural_Language_Processing/p2_transformers/weights"
  load_name: "ckpt_best_transformer_TinyStories_exp_bs64_cosine_warmup4000_total20000_lrmax0.007_lrmin0.0014_20250724_061642.pt"
tokenizer:
  special_tokens: ["<|endoftext|>"]
  files_path: "~/ai_projects/data/TinyStoriesV2-GPT4-train"
max_num_tokens: 100
