{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2eee84c-80ee-4e6e-9416-aa5ccd9e6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from p2_efficiency.utils import benchmark_llm, profile_llm_memory\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85efbeda-60ea-4bb0-b0ec-aabc4797b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"size\": [\"small\", \"medium\", \"large\", \"xl\", \"2.7B\"], \n",
    "    \"num_layers\": [12, 24, 36, 48, 32],\n",
    "    \"d_model\": [768, 1024, 1280, 1600, 2560],\n",
    "    \"d_ff\": [3072, 4096, 5120, 6400, 10240],\n",
    "    \"num_heads\": [12, 16, 20, 25, 32],\n",
    "    \"cntx_len\": [],\n",
    "    \"mode\": [],\n",
    "    \"mean (milliseconds)\": [],\n",
    "    \"std (milliseconds)\": []\n",
    "}\n",
    "cntx_lens = [128, 256, 512, 1024]\n",
    "modes = [\"forward\", \"forward+backward\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2748b-bb99-4405-9093-d206fccf8b87",
   "metadata": {},
   "source": [
    "# Benchmark speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0547521e-b981-4590-a629-6f17fff2536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(inputs, warmup_iters = 5):\n",
    "    data_benchmark = {k:[] for k in inputs}\n",
    "    for i in range(len(inputs[\"size\"])):\n",
    "        for cntx_len in cntx_lens:\n",
    "            for mode in modes:    \n",
    "                try:\n",
    "                    m, std = benchmark_llm(\n",
    "                        d_model = inputs[\"d_model\"][i], d_ff = inputs[\"d_ff\"][i], num_layers = inputs[\"num_layers\"][i], num_heads = inputs[\"num_heads\"][i],\n",
    "                        context_length = cntx_len, warmup_iters = warmup_iters, benchmark_iters = 10, mode = mode\n",
    "                    )\n",
    "                    for k in inputs:\n",
    "                        if inputs[k] != []:\n",
    "                            data_benchmark[k].append(inputs[k][i])\n",
    "                    data_benchmark[\"cntx_len\"].append(cntx_len)\n",
    "                    data_benchmark[\"mode\"].append(mode)\n",
    "                    data_benchmark[\"mean (milliseconds)\"].append(m)\n",
    "                    data_benchmark[\"std (milliseconds)\"].append(std)\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print(f\"⚠️ OOM for {inputs[\"size\"][i]} model with context length = {cntx_len}, freeing memory...\")\n",
    "                        torch.cuda.empty_cache()   # frees cached allocator blocks\n",
    "                        gc.collect()               # run Python garbage collector\n",
    "                        torch.cuda.synchronize()   # wait for cleanup to complete\n",
    "                    else:\n",
    "                        raise e\n",
    "                sleep(3)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return data_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a96c44c-4860-49d2-9680-80fec652d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 512, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 128, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 256, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "CPU times: user 26.9 s, sys: 4.78 s, total: 31.7 s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_benchmark0 = run_benchmark(inputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd74e75-bcc1-46fb-b353-1f814ec12c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>d_model</th>\n",
       "      <th>d_ff</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>cntx_len</th>\n",
       "      <th>mode</th>\n",
       "      <th>mean (milliseconds)</th>\n",
       "      <th>std (milliseconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>27.210902</td>\n",
       "      <td>49.409920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>31.933658</td>\n",
       "      <td>26.380630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>11.677848</td>\n",
       "      <td>1.127935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>27.421579</td>\n",
       "      <td>0.057029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>17.287351</td>\n",
       "      <td>1.050202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>49.400880</td>\n",
       "      <td>0.045378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward</td>\n",
       "      <td>54.004190</td>\n",
       "      <td>0.434013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>158.102872</td>\n",
       "      <td>0.055844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>20.932854</td>\n",
       "      <td>0.370619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>49.802970</td>\n",
       "      <td>0.117829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>25.096372</td>\n",
       "      <td>0.221589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>68.336893</td>\n",
       "      <td>0.091793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>46.317889</td>\n",
       "      <td>0.149245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>143.484002</td>\n",
       "      <td>0.179586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>37.584072</td>\n",
       "      <td>0.338825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>92.598348</td>\n",
       "      <td>0.313195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>48.860417</td>\n",
       "      <td>0.197055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>135.550368</td>\n",
       "      <td>0.263071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>315.227530</td>\n",
       "      <td>0.342933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>59.102478</td>\n",
       "      <td>0.297637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>158.399039</td>\n",
       "      <td>0.388924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>88.823623</td>\n",
       "      <td>0.409204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>256.437199</td>\n",
       "      <td>0.510234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>68.703935</td>\n",
       "      <td>0.410888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>125.658667</td>\n",
       "      <td>1.689442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size  num_layers  d_model   d_ff  num_heads  cntx_len              mode  \\\n",
       "0    small          12      768   3072         12       128           forward   \n",
       "1    small          12      768   3072         12       128  forward+backward   \n",
       "2    small          12      768   3072         12       256           forward   \n",
       "3    small          12      768   3072         12       256  forward+backward   \n",
       "4    small          12      768   3072         12       512           forward   \n",
       "5    small          12      768   3072         12       512  forward+backward   \n",
       "6    small          12      768   3072         12      1024           forward   \n",
       "7    small          12      768   3072         12      1024  forward+backward   \n",
       "8   medium          24     1024   4096         16       128           forward   \n",
       "9   medium          24     1024   4096         16       128  forward+backward   \n",
       "10  medium          24     1024   4096         16       256           forward   \n",
       "11  medium          24     1024   4096         16       256  forward+backward   \n",
       "12  medium          24     1024   4096         16       512           forward   \n",
       "13  medium          24     1024   4096         16       512  forward+backward   \n",
       "14   large          36     1280   5120         20       128           forward   \n",
       "15   large          36     1280   5120         20       128  forward+backward   \n",
       "16   large          36     1280   5120         20       256           forward   \n",
       "17   large          36     1280   5120         20       256  forward+backward   \n",
       "18   large          36     1280   5120         20       512  forward+backward   \n",
       "19      xl          48     1600   6400         25       128           forward   \n",
       "20      xl          48     1600   6400         25       128  forward+backward   \n",
       "21      xl          48     1600   6400         25       256           forward   \n",
       "22      xl          48     1600   6400         25       256  forward+backward   \n",
       "23    2.7B          32     2560  10240         32       128           forward   \n",
       "24    2.7B          32     2560  10240         32       256           forward   \n",
       "\n",
       "    mean (milliseconds)  std (milliseconds)  \n",
       "0             27.210902           49.409920  \n",
       "1             31.933658           26.380630  \n",
       "2             11.677848            1.127935  \n",
       "3             27.421579            0.057029  \n",
       "4             17.287351            1.050202  \n",
       "5             49.400880            0.045378  \n",
       "6             54.004190            0.434013  \n",
       "7            158.102872            0.055844  \n",
       "8             20.932854            0.370619  \n",
       "9             49.802970            0.117829  \n",
       "10            25.096372            0.221589  \n",
       "11            68.336893            0.091793  \n",
       "12            46.317889            0.149245  \n",
       "13           143.484002            0.179586  \n",
       "14            37.584072            0.338825  \n",
       "15            92.598348            0.313195  \n",
       "16            48.860417            0.197055  \n",
       "17           135.550368            0.263071  \n",
       "18           315.227530            0.342933  \n",
       "19            59.102478            0.297637  \n",
       "20           158.399039            0.388924  \n",
       "21            88.823623            0.409204  \n",
       "22           256.437199            0.510234  \n",
       "23            68.703935            0.410888  \n",
       "24           125.658667            1.689442  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_benchmark0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2027dd1-b86c-44ad-8402-b1867b25c45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 512, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 128, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 256, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "CPU times: user 28.8 s, sys: 5.23 s, total: 34 s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_benchmark1 = run_benchmark(inputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0dcf756-f2c2-48d0-aaf2-6651eead0e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>d_model</th>\n",
       "      <th>d_ff</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>cntx_len</th>\n",
       "      <th>mode</th>\n",
       "      <th>mean (milliseconds)</th>\n",
       "      <th>std (milliseconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>10.715511</td>\n",
       "      <td>1.201898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>22.352556</td>\n",
       "      <td>0.697683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>11.609577</td>\n",
       "      <td>1.111441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>27.318731</td>\n",
       "      <td>0.055945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>16.981092</td>\n",
       "      <td>0.771432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>49.322208</td>\n",
       "      <td>0.014699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward</td>\n",
       "      <td>53.888881</td>\n",
       "      <td>0.408823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>157.983309</td>\n",
       "      <td>0.023634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>20.897351</td>\n",
       "      <td>0.384320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>49.843070</td>\n",
       "      <td>0.062830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>25.043434</td>\n",
       "      <td>0.098220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>68.278197</td>\n",
       "      <td>0.083438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>46.217995</td>\n",
       "      <td>0.146520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>143.184884</td>\n",
       "      <td>0.112809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>37.585484</td>\n",
       "      <td>0.401018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>92.532466</td>\n",
       "      <td>0.143374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>48.673294</td>\n",
       "      <td>0.068571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>135.303369</td>\n",
       "      <td>0.202843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>314.812979</td>\n",
       "      <td>0.331153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>59.092931</td>\n",
       "      <td>0.240842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>158.378071</td>\n",
       "      <td>0.252248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>88.684912</td>\n",
       "      <td>0.349997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>255.540737</td>\n",
       "      <td>0.258342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>68.572935</td>\n",
       "      <td>0.380293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>125.735622</td>\n",
       "      <td>0.758756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size  num_layers  d_model   d_ff  num_heads  cntx_len              mode  \\\n",
       "0    small          12      768   3072         12       128           forward   \n",
       "1    small          12      768   3072         12       128  forward+backward   \n",
       "2    small          12      768   3072         12       256           forward   \n",
       "3    small          12      768   3072         12       256  forward+backward   \n",
       "4    small          12      768   3072         12       512           forward   \n",
       "5    small          12      768   3072         12       512  forward+backward   \n",
       "6    small          12      768   3072         12      1024           forward   \n",
       "7    small          12      768   3072         12      1024  forward+backward   \n",
       "8   medium          24     1024   4096         16       128           forward   \n",
       "9   medium          24     1024   4096         16       128  forward+backward   \n",
       "10  medium          24     1024   4096         16       256           forward   \n",
       "11  medium          24     1024   4096         16       256  forward+backward   \n",
       "12  medium          24     1024   4096         16       512           forward   \n",
       "13  medium          24     1024   4096         16       512  forward+backward   \n",
       "14   large          36     1280   5120         20       128           forward   \n",
       "15   large          36     1280   5120         20       128  forward+backward   \n",
       "16   large          36     1280   5120         20       256           forward   \n",
       "17   large          36     1280   5120         20       256  forward+backward   \n",
       "18   large          36     1280   5120         20       512  forward+backward   \n",
       "19      xl          48     1600   6400         25       128           forward   \n",
       "20      xl          48     1600   6400         25       128  forward+backward   \n",
       "21      xl          48     1600   6400         25       256           forward   \n",
       "22      xl          48     1600   6400         25       256  forward+backward   \n",
       "23    2.7B          32     2560  10240         32       128           forward   \n",
       "24    2.7B          32     2560  10240         32       256           forward   \n",
       "\n",
       "    mean (milliseconds)  std (milliseconds)  \n",
       "0             10.715511            1.201898  \n",
       "1             22.352556            0.697683  \n",
       "2             11.609577            1.111441  \n",
       "3             27.318731            0.055945  \n",
       "4             16.981092            0.771432  \n",
       "5             49.322208            0.014699  \n",
       "6             53.888881            0.408823  \n",
       "7            157.983309            0.023634  \n",
       "8             20.897351            0.384320  \n",
       "9             49.843070            0.062830  \n",
       "10            25.043434            0.098220  \n",
       "11            68.278197            0.083438  \n",
       "12            46.217995            0.146520  \n",
       "13           143.184884            0.112809  \n",
       "14            37.585484            0.401018  \n",
       "15            92.532466            0.143374  \n",
       "16            48.673294            0.068571  \n",
       "17           135.303369            0.202843  \n",
       "18           314.812979            0.331153  \n",
       "19            59.092931            0.240842  \n",
       "20           158.378071            0.252248  \n",
       "21            88.684912            0.349997  \n",
       "22           255.540737            0.258342  \n",
       "23            68.572935            0.380293  \n",
       "24           125.735622            0.758756  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_benchmark1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802289a0-e38c-4b98-bd82-a75269620b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for medium model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 512, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for large model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 512, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for xl model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 128, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 256, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 512, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "⚠️ OOM for 2.7B model with context length = 1024, freeing memory...\n",
      "CPU times: user 37.3 s, sys: 6.62 s, total: 44 s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_benchmark5 = run_benchmark(inputs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d5e4a2-4445-4264-b5b9-83cae4af7e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>d_model</th>\n",
       "      <th>d_ff</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>cntx_len</th>\n",
       "      <th>mode</th>\n",
       "      <th>mean (milliseconds)</th>\n",
       "      <th>std (milliseconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>10.248798</td>\n",
       "      <td>0.031703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>22.193910</td>\n",
       "      <td>0.035818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>11.253574</td>\n",
       "      <td>0.027241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>27.365297</td>\n",
       "      <td>0.056862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>16.762400</td>\n",
       "      <td>0.024275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>49.391200</td>\n",
       "      <td>0.012920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward</td>\n",
       "      <td>53.803942</td>\n",
       "      <td>0.074543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>small</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>3072</td>\n",
       "      <td>12</td>\n",
       "      <td>1024</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>158.165682</td>\n",
       "      <td>0.060323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>20.782726</td>\n",
       "      <td>0.055863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>50.035197</td>\n",
       "      <td>0.072863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>25.098731</td>\n",
       "      <td>0.092298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>68.450864</td>\n",
       "      <td>0.059805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward</td>\n",
       "      <td>46.550062</td>\n",
       "      <td>0.091348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>medium</td>\n",
       "      <td>24</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>143.713546</td>\n",
       "      <td>0.101595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>37.508312</td>\n",
       "      <td>0.129265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>92.789625</td>\n",
       "      <td>0.114888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>48.943816</td>\n",
       "      <td>0.093431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>135.637717</td>\n",
       "      <td>0.075010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>large</td>\n",
       "      <td>36</td>\n",
       "      <td>1280</td>\n",
       "      <td>5120</td>\n",
       "      <td>20</td>\n",
       "      <td>512</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>316.131831</td>\n",
       "      <td>0.247579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>59.427696</td>\n",
       "      <td>0.542112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>158.707638</td>\n",
       "      <td>0.105529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>89.402820</td>\n",
       "      <td>0.165109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>xl</td>\n",
       "      <td>48</td>\n",
       "      <td>1600</td>\n",
       "      <td>6400</td>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>forward+backward</td>\n",
       "      <td>257.130595</td>\n",
       "      <td>0.169599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>forward</td>\n",
       "      <td>69.238860</td>\n",
       "      <td>0.094211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.7B</td>\n",
       "      <td>32</td>\n",
       "      <td>2560</td>\n",
       "      <td>10240</td>\n",
       "      <td>32</td>\n",
       "      <td>256</td>\n",
       "      <td>forward</td>\n",
       "      <td>127.043114</td>\n",
       "      <td>0.132177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size  num_layers  d_model   d_ff  num_heads  cntx_len              mode  \\\n",
       "0    small          12      768   3072         12       128           forward   \n",
       "1    small          12      768   3072         12       128  forward+backward   \n",
       "2    small          12      768   3072         12       256           forward   \n",
       "3    small          12      768   3072         12       256  forward+backward   \n",
       "4    small          12      768   3072         12       512           forward   \n",
       "5    small          12      768   3072         12       512  forward+backward   \n",
       "6    small          12      768   3072         12      1024           forward   \n",
       "7    small          12      768   3072         12      1024  forward+backward   \n",
       "8   medium          24     1024   4096         16       128           forward   \n",
       "9   medium          24     1024   4096         16       128  forward+backward   \n",
       "10  medium          24     1024   4096         16       256           forward   \n",
       "11  medium          24     1024   4096         16       256  forward+backward   \n",
       "12  medium          24     1024   4096         16       512           forward   \n",
       "13  medium          24     1024   4096         16       512  forward+backward   \n",
       "14   large          36     1280   5120         20       128           forward   \n",
       "15   large          36     1280   5120         20       128  forward+backward   \n",
       "16   large          36     1280   5120         20       256           forward   \n",
       "17   large          36     1280   5120         20       256  forward+backward   \n",
       "18   large          36     1280   5120         20       512  forward+backward   \n",
       "19      xl          48     1600   6400         25       128           forward   \n",
       "20      xl          48     1600   6400         25       128  forward+backward   \n",
       "21      xl          48     1600   6400         25       256           forward   \n",
       "22      xl          48     1600   6400         25       256  forward+backward   \n",
       "23    2.7B          32     2560  10240         32       128           forward   \n",
       "24    2.7B          32     2560  10240         32       256           forward   \n",
       "\n",
       "    mean (milliseconds)  std (milliseconds)  \n",
       "0             10.248798            0.031703  \n",
       "1             22.193910            0.035818  \n",
       "2             11.253574            0.027241  \n",
       "3             27.365297            0.056862  \n",
       "4             16.762400            0.024275  \n",
       "5             49.391200            0.012920  \n",
       "6             53.803942            0.074543  \n",
       "7            158.165682            0.060323  \n",
       "8             20.782726            0.055863  \n",
       "9             50.035197            0.072863  \n",
       "10            25.098731            0.092298  \n",
       "11            68.450864            0.059805  \n",
       "12            46.550062            0.091348  \n",
       "13           143.713546            0.101595  \n",
       "14            37.508312            0.129265  \n",
       "15            92.789625            0.114888  \n",
       "16            48.943816            0.093431  \n",
       "17           135.637717            0.075010  \n",
       "18           316.131831            0.247579  \n",
       "19            59.427696            0.542112  \n",
       "20           158.707638            0.105529  \n",
       "21            89.402820            0.165109  \n",
       "22           257.130595            0.169599  \n",
       "23            69.238860            0.094211  \n",
       "24           127.043114            0.132177  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_benchmark5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe35788-69ac-446e-b7b5-97a72ad30774",
   "metadata": {},
   "source": [
    "# Profile memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbf299c-797a-46e2-bd4e-6af928edbf62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1016 17:48:57.472963150 unwind.cpp:219] Warning: Unsupported unwinding pattern: unknown op code 0x8 (function unwinderFor)\n",
      "[W1016 17:48:57.504304671 unwind.cpp:219] Warning: Unsupported unwinding pattern: unknown op code 0x8 (function unwinderFor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.5 s, sys: 4.54 s, total: 43 s\n",
      "Wall time: 43.7 s\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 8.81 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.45 GiB is allocated by PyTorch, and 3.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfor i in range(5):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    d_model, d_ff, num_layers, num_heads = info[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43md_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][i], info[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43md_ff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][i], info[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][i], info[\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_heads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m][i]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    profile_llm_memory(d_model = d_model, d_ff = d_ff, num_layers = num_layers , num_heads = num_heads)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/EfZ6W7McNkEXZ4aUIENTZ/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/EfZ6W7McNkEXZ4aUIENTZ/lib/python3.12/site-packages/IPython/core/magics/execution.py:1452\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/uv/archive-v0/EfZ6W7McNkEXZ4aUIENTZ/lib/python3.12/site-packages/IPython/core/magics/execution.py:1416\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1414\u001b[39m st = clock2()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:3\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/p2_efficiency/utils/profiling.py:204\u001b[39m, in \u001b[36mprofile_llm_memory\u001b[39m\u001b[34m(d_model, d_ff, num_layers, num_heads, context_length, warmup_iters, profile_iters, mode)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    203\u001b[39m         loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m         \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Save a pickle file to be loaded by PyTorch's online tool.\u001b[39;00m\n\u001b[32m    207\u001b[39m torch.cuda.memory._dump_snapshot(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmemory_snapshot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_ff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pickle\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ai_projects/llms-from-scratch/.venv/lib/python3.12/site-packages/torch/optim/adam.py:773\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    771\u001b[39m     exp_avg_sq_sqrt = torch._foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     exp_avg_sq_sqrt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    775\u001b[39m torch._foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[32m    776\u001b[39m torch._foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 31.37 GiB of which 8.81 MiB is free. Including non-PyTorch memory, this process has 31.34 GiB memory in use. Of the allocated memory 27.45 GiB is allocated by PyTorch, and 3.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(5):\n",
    "    d_model, d_ff, num_layers, num_heads = info[\"d_model\"][i], info[\"d_ff\"][i], info[\"num_layers\"][i], info[\"num_heads\"][i]\n",
    "    profile_llm_memory(d_model = d_model, d_ff = d_ff, num_layers = num_layers , num_heads = num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976e2b86-3109-46e0-9317-e800b30f3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasha/.cache/uv/builds-v0/.tmp0pyFvl/bin/python\n",
      "/home/sasha/.cache/uv/builds-v0/.tmp0pyFvl/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad3f1d-aad8-47b3-9b28-1df1c5ba1015",
   "metadata": {},
   "source": [
    "# Mixed Precision (AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00baa3ad-c0ff-424a-a8c1-9f76e47d2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f50741-5e1d-4d61-b6df-397953b09730",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89d96e7-70c6-42f2-84b8-e66058401ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0001)\n"
     ]
    }
   ],
   "source": [
    "# accumulating float32 in float32\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67876729-41b8-429d-8829-7010fef23e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9531, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# accumulating float16 in float16\n",
    "s = torch.tensor(0, dtype=torch.float16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b7fc8e-4507-4b22-a94d-897b74aedf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "# accumulating float16 in float32\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.float16)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edfab32d-bbf1-456e-9e79-c5e11e1e354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0021)\n"
     ]
    }
   ],
   "source": [
    "# accumulating float16->float32 in float32\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    x = torch.tensor(0.01,dtype=torch.float16)\n",
    "    s += x.type(torch.float32)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c915636-b538-4c46-91d9-31bca12f96a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# accumulating bfloat16 in bfloat16\n",
    "s = torch.tensor(0, dtype=torch.bfloat16)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.bfloat16)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a362f7-b688-40bf-8ad7-df2aaf689b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.0098)\n"
     ]
    }
   ],
   "source": [
    "# accumulating bfloat16 in float32\n",
    "s = torch.tensor(0, dtype=torch.float32)\n",
    "for i in range(1000):\n",
    "    s += torch.tensor(0.01,dtype=torch.bfloat16)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb2e30-bdf8-434b-ace5-d1ee427567b0",
   "metadata": {},
   "source": [
    "## Understanding Toy Model dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463ce78f-0a7a-4a55-92b4-bd6d6d1af6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"       input dtype:\", x.dtype)\n",
    "        x = self.fc1(x)\n",
    "        print(\" fc1 activat dtype:\", x.dtype)\n",
    "        x = self.relu(x)\n",
    "        print(\"relu activat dtype:\", x.dtype)\n",
    "        x = self.ln(x)\n",
    "        print(\"  ln activat dtype:\", x.dtype)\n",
    "        x = self.fc2(x)\n",
    "        print(\" fc2 activat dtype:\", x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62d4968-436b-4ee4-985d-0cb3f72862cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda:0\")\n",
    "in_features, out_features = 160, 32\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af30ab0b-b7a4-45f5-9676-63613dfa1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model = ToyModel(in_features, out_features)\n",
    "model.to(dtype = dtype, device = device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a67949-25ad-46d6-bb2f-5e97d53f1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight        torch.float32 torch.Size([10, 160])\n",
      "ln.weight         torch.float32 torch.Size([10])\n",
      "ln.bias           torch.float32 torch.Size([10])\n",
      "fc2.weight        torch.float32 torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(name, \" \" * (16 - len(name)), p.dtype, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36b09b-24a6-4587-8207-5d2a249cbcff",
   "metadata": {},
   "source": [
    "### float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3df0c2e6-b3f0-4129-931b-4720f33ae8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fc1 weights dtype: torch.float32\n",
      " fc2 weights dtype: torch.float32\n",
      "  ln weights dtype: torch.float32\n",
      "  ln    bias dtype: torch.float32 \n",
      "\n",
      "       input dtype: torch.float32\n",
      " fc1 activat dtype: torch.float16\n",
      "relu activat dtype: torch.float16\n",
      "  ln activat dtype: torch.float32\n",
      " fc2 activat dtype: torch.float16\n",
      "\n",
      "      logits dtype: torch.float16\n",
      "\n",
      "        loss dtype: torch.float32\n",
      " fc1   grads dtype: torch.float32\n",
      " fc2   grads dtype: torch.float32\n",
      "  ln  wgrads dtype: torch.float32\n",
      "  ln  bgrads dtype: torch.float32 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "x = torch.rand(batch_size, in_features, dtype = dtype, device = device)\n",
    "y = torch.empty(batch_size, dtype=torch.long, device = device).random_(out_features)\n",
    "for i in range(1):\n",
    "    with autocast('cuda', enabled = True, dtype=torch.float16):\n",
    "        print(\" fc1 weights dtype:\", model.fc1.weight.dtype)\n",
    "        print(\" fc2 weights dtype:\", model.fc2.weight.dtype)\n",
    "        print(\"  ln weights dtype:\", model.ln.weight.dtype)\n",
    "        print(\"  ln    bias dtype:\", model.ln.bias.dtype, \"\\n\")\n",
    "        logits = model(x)\n",
    "        print(\"\\n      logits dtype:\", logits.dtype)\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_fn(logits, y)\n",
    "        print(\"\\n        loss dtype:\", loss.dtype)\n",
    "\n",
    "    # scale (and make optimizer step)\n",
    "    scaler.scale(loss).backward()\n",
    "    # print grad\n",
    "    print(\" fc1   grads dtype:\", model.fc1.weight.grad.dtype)\n",
    "    print(\" fc2   grads dtype:\", model.fc2.weight.grad.dtype)\n",
    "    print(\"  ln  wgrads dtype:\", model.ln.weight.grad.dtype)\n",
    "    print(\"  ln  bgrads dtype:\", model.ln.bias.grad.dtype,\"\\n\")\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcc1c9-b954-456b-990a-3d0609b8070e",
   "metadata": {},
   "source": [
    "### bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38371eee-9408-4457-b191-3482e5b275a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fc1 weights dtype: torch.float32\n",
      " fc2 weights dtype: torch.float32\n",
      "  ln weights dtype: torch.float32\n",
      "  ln    bias dtype: torch.float32 \n",
      "\n",
      "       input dtype: torch.float32\n",
      " fc1 activat dtype: torch.bfloat16\n",
      "relu activat dtype: torch.bfloat16\n",
      "  ln activat dtype: torch.float32\n",
      " fc2 activat dtype: torch.bfloat16\n",
      "\n",
      "      logits dtype: torch.bfloat16\n",
      "\n",
      "        loss dtype: torch.float32\n",
      " fc1   grads dtype: torch.float32\n",
      " fc2   grads dtype: torch.float32\n",
      "  ln  wgrads dtype: torch.float32\n",
      "  ln  bgrads dtype: torch.float32 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "x = torch.rand(batch_size, in_features, dtype = dtype, device = device)\n",
    "y = torch.empty(batch_size, dtype=torch.long, device = device).random_(out_features)\n",
    "for i in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    with autocast('cuda', enabled = True, dtype=torch.bfloat16):\n",
    "        print(\" fc1 weights dtype:\", model.fc1.weight.dtype)\n",
    "        print(\" fc2 weights dtype:\", model.fc2.weight.dtype)\n",
    "        print(\"  ln weights dtype:\", model.ln.weight.dtype)\n",
    "        print(\"  ln    bias dtype:\", model.ln.bias.dtype,\"\\n\")\n",
    "        logits = model(x)\n",
    "        print(\"\\n      logits dtype:\", logits.dtype)\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_fn(logits, y)\n",
    "        print(\"\\n        loss dtype:\", loss.dtype)\n",
    "\n",
    "    # scale (and make optimizer step)\n",
    "    scaler.scale(loss).backward()\n",
    "    # print grad\n",
    "    print(\" fc1   grads dtype:\", model.fc1.weight.grad.dtype)\n",
    "    print(\" fc2   grads dtype:\", model.fc2.weight.grad.dtype)\n",
    "    print(\"  ln  wgrads dtype:\", model.ln.weight.grad.dtype)\n",
    "    print(\"  ln  bgrads dtype:\", model.ln.bias.grad.dtype,\"\\n\")\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aded686-2afa-43d1-9bf4-d4d7d3de85a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
